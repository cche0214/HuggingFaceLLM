{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmV2FovEyp28TWPwo8R2dQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cche0214/HuggingFaceLLM/blob/main/05Datasets%E5%BA%93.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhvODiIRNyZg"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
      ],
      "metadata": {
        "id": "znPcSTLjEYUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "BFN9QvnCEbIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -dkv SQuAD_it-*.json.gz"
      ],
      "metadata": {
        "id": "aDCJCnccEePK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "soLZ3g-FEhBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {\n",
        "#   \"version\": \"1.1\",\n",
        "#   \"data\": [\n",
        "#     {\n",
        "#       \"title\": \"Super_Bowl_50\",\n",
        "#       \"paragraphs\": [\n",
        "#         {\n",
        "#           \"context\": \"Super Bowl 50 was an American football game...\",\n",
        "#           \"qas\": [\n",
        "#             {\n",
        "#               \"id\": \"56be4db0acb8001400a502ec\",\n",
        "#               \"question\": \"Which NFL team represented the AFC?\",\n",
        "#               \"answers\": [\n",
        "#                 {\"text\": \"Denver Broncos\", \"answer_start\": 177}\n",
        "#               ]\n",
        "#             }\n",
        "#           ]\n",
        "#         }\n",
        "#       ]\n",
        "#     }\n",
        "#   ]\n",
        "# }\n",
        "# 嵌套字典，样本也就是数据可能在某个字段中"
      ],
      "metadata": {
        "id": "kK9myf7MEjep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {\"text\": \"I love NLP\", \"label\": 1}\n",
        "# {\"text\": \"I hate bugs\", \"label\": 0}\n",
        "# {\"text\": \"Transformers are powerful\", \"label\": 1}\n",
        "# JSON Lines，每一行都是一个JSON，相当于每一行就是一条数据"
      ],
      "metadata": {
        "id": "KdCkXhhYFBi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQuAD-it是嵌套字典，字段存储在data字段里面，field参数的含义是指定从哪个字段取出样本列表\n",
        "from datasets import load_dataset\n",
        "\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
      ],
      "metadata": {
        "id": "Y1uWpJOtFeYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载本地文件会创建一个带有train标签的DatasetDict对象\n",
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "NWnCnz2RF0dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "k4vGvhqsF3gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这一条数据的格式是一段文章+一个问题+一个回答的格式\n",
        "squad_it_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "I1lKOkosGDEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过向data_files参数传入一个字典，构造同时包含训练集和验证集的一个DatasetDict对象\n",
        "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\" : \"SQuAD_it-test.json\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "aCAPFK0TGGrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "nEyZrKy1HcaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets甚至可以自动解压文件，你可以跳过一开始的命令行解压方式\n",
        "data_files = {\"trian\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "-8sHO3ALHdql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "UbfFsBAYH0Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载远程文件和加载本地文件一样简单，只需要换成url\n",
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ],
      "metadata": {
        "id": "-hyNolQEH10J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad_it_dataset"
      ],
      "metadata": {
        "id": "d-BFaY7oH_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\""
      ],
      "metadata": {
        "id": "e6i4nDRPIDA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drugsCom_raw.zip"
      ],
      "metadata": {
        "id": "hvxwijroIOyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "fDbPEKP7IRkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# tsv是vsc的一个变体，它使用制表符作为分隔符而不是逗号，因此可以用csv的处理方式，指定分隔符delimiter\n",
        "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
        "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
      ],
      "metadata": {
        "id": "MgoNlv1eISpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset"
      ],
      "metadata": {
        "id": "R0O9gkO1IkiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "qcpyS0taJDJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "Oh6QFzBSJEye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][1]"
      ],
      "metadata": {
        "id": "g4yi6QuNJHK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][100]"
      ],
      "metadata": {
        "id": "d8RH6l3JJfiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以通过Dataset.shuffle()和Dataset.select()函数创建一个随机的样本\n",
        "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "drug_sample[:3]"
      ],
      "metadata": {
        "id": "B8kdyZ3ZJOTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这里是在验证Unnamed:0是不是就是患者的ID，把训练集和验证集的数据长度，和这个字段的唯一值比较，相等就说明其实就是患者ID\n",
        "# assert 条件 意思是如果条件为True，那就什么都不发生，如果为False，那就立即报错\n",
        "for split in drug_dataset.keys():\n",
        "  assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
      ],
      "metadata": {
        "id": "ZEUi0z0OJ0Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset.keys()"
      ],
      "metadata": {
        "id": "lhQnPZcrKhAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset"
      ],
      "metadata": {
        "id": "zkF_I3tZK0_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(drug_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "XrJ3khxUL1A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 确定了Unnamed:0就是患者ID，可以通过DatasetDict.rename_column()进行重命名\n",
        "drug_dataset = drug_dataset.rename_column(\n",
        "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
        ")"
      ],
      "metadata": {
        "id": "tVNhy3uoL2YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset"
      ],
      "metadata": {
        "id": "bR09CtlLMjge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特定药物和病症的数量\n",
        "for split in drug_dataset.keys():\n",
        "  print(len(drug_dataset[split].unique(\"drugName\")))\n",
        "  print(len(drug_dataset[split].unique(\"condition\")))"
      ],
      "metadata": {
        "id": "ByuPeTREMl77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "cC5p5ug4Mu79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 有关于这个map函数的设计，我们从他处理的数据结构开始理解，也就是Dataset.map()\n",
        "# Dataset本质是一个保存了很多dict的集合，比如说下面这个是Dataset的一条数据，是一个dict\n",
        "# 所以Dataset.map()函数的意思就是，对于Dataset里面的每一条数据，也就是每一个dict，运用给定map函数的函数\n",
        "# 比如下面定义的函数lowercase_condition(example)，根据上面我们知道，这里的example其实就是Dataset里面的一个样本，也就是一个dict\n",
        "# map要求你的函数返回值必须是一个dict，并且只需要返回你需要修改或者新增的几个keyvalue就好，其他过程全部交给map函数处理\n",
        "drug_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "4zc8N0UiPDsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_condition(example):\n",
        "    return {\"condition\": example[\"condition\"].lower()}\n"
      ],
      "metadata": {
        "id": "jcNbD4q4NaCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 报错说明confition列存在None，不能转换为小写，因为它们不是字符串\n",
        "drug_dataset.map(lowercase_condition)"
      ],
      "metadata": {
        "id": "1l3-mpkxNiCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以把存在None的行直接删掉，用Dataset.filter()删除，类似Dataset.map()\n",
        "# 类似的意思就是，也需要定义函数，传给filter调用，逐行处理样本\n",
        "# filter是筛选行，对于每一行，返回True保留这一行，返回False删除这一行\n",
        "# 因此你写的函数返回必须是一个bool变量\n",
        "def filter_nones(x):\n",
        "  return x[\"condition\"] is not None"
      ],
      "metadata": {
        "id": "BIJrIahqNmJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以发现两个数据集的规模都变小了\n",
        "drug_dataset = drug_dataset.filter(filter_nones)\n",
        "drug_dataset"
      ],
      "metadata": {
        "id": "Z3ncFzdNQ0a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.map(lowercase_condition)"
      ],
      "metadata": {
        "id": "Bn8eZxqmR3pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][\"condition\"][:3]"
      ],
      "metadata": {
        "id": "gtLpEF3bR_XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个函数统计review评论这一列有多少个单词\n",
        "# 返回的key不在数据里面的任何一个key，所以他会给每一个数据加上一个新的key\n",
        "def compute_review_length(example):\n",
        "  return {\"review_length\": len(example[\"review\"].split())}"
      ],
      "metadata": {
        "id": "ZXlN1qCsS9pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.map(compute_review_length)"
      ],
      "metadata": {
        "id": "mV2e0WY_TSso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "Dp3kVC1uToB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看极端评论的长度，用Dataset.sort()排序，是升序，也就是从小到大\n",
        "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
      ],
      "metadata": {
        "id": "gTcwt6cyTwpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 删除长度小于三十的评论，用filter函数，不过这里直接用匿名函数更方便\n",
        "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)"
      ],
      "metadata": {
        "id": "s8VpnF9CUjC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 删除之后，数据集的规模又小了一点\n",
        "drug_dataset"
      ],
      "metadata": {
        "id": "ep-4I_Q-U3zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理评论里面的HTML字符，原理如下\n",
        "import html\n",
        "\n",
        "text = \"I&#039;m a transformer called BERT\"\n",
        "html.unescape(text)"
      ],
      "metadata": {
        "id": "iKcdmX4FVKMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
      ],
      "metadata": {
        "id": "dF00DO6bXJm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map()方法的加速，batched=True\n",
        "# 原本对于输入map()的函数，都是对于一条数据的操作，比如说下面的一个字典\n",
        "# {\"text\": \"hello\", \"label\": 1}\n",
        "# 如果调用map()的时候加上batched=True参数，就从对一条样本的操作变成一个列表的操作\n",
        "# {\n",
        "#   \"text\": [\"hello\", \"hi\", \"good morning\", ...],   # 长度 = batch_size\n",
        "#   \"label\": [1, 0, 1, ...]\n",
        "# }\n",
        "# 竖着看，就是一个样本，然后横着走\n",
        "# 因此对于map()要调用的函数，他的输入和返回值也要改变，用python的列表推导式返回值\n",
        "# result = [x.lower() for x in texts]\n",
        "# 输入就变成了一个batch，他的格式就类似上面的列表推导式，同样的你返回也要返回一个列表推导式\n",
        "# def lowercase_batch(batch):\n",
        "#     return {\n",
        "#         \"condition\": [c.lower() for c in batch[\"condition\"]]\n",
        "#     }\n",
        "# batch现在是一个key value(list)的字典，所以batch[\"condition\"]拿到的是一个list，对这个list里面的元素c（代表一个样本的数据)进行处理之后，再返回列表给\"condition\"字段"
      ],
      "metadata": {
        "id": "OtCNCGr8XaBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快的一比，batched默认1000，这里的意思就是x是一个batch，就是上面的\n",
        "# {\n",
        "#   \"text\": [\"hello\", \"hi\", \"good morning\", ...],   # 长度 = batch_size\n",
        "#   \"label\": [1, 0, 1, ...]\n",
        "# }\n",
        "# 从里面找review字段，是一个list，把里面的每一个元素o去掉html字符，在通过列表推导式赋给review这个key，因为加入batch之后，这个key就是value(list)\n",
        "new_drug_dataset = drug_dataset.map(\n",
        "    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "j2WUiZoqZqdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快速tokenizer，一般不指定参数直接下载的都是快速tokenizer\n",
        "# 比较快速/慢速tokenizer和加不加batch对同一文本的编码速度\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True)"
      ],
      "metadata": {
        "id": "2iYE5v1oZ9K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这里教程里面写了很乱，我总结一下\n",
        "# 在from_pretrained()里面可以通过参数use_fast选择导入的是快速tokenizer还是慢速，默认就是快速的\n",
        "# tokenizer是可以处理list数据的，就是同时对多条样本数据进行tokenize\n",
        "# 前面说map函数也可以开启batched=True来进行加速，就是处理list结构的数据\n",
        "# 所以如果你不用map的batched，那么每次tokenize的都是一条一条样本来的\n",
        "# 如果开启了，就可以结合tokenizer的特性，对batch进行tokenize，也就是list处理\n",
        "# 因此快速tokenizer加上batched=True是最快的"
      ],
      "metadata": {
        "id": "cgbFJduIhGsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快速tokenizer加上Batch=True\n",
        "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "2VBoWkVab5U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快速tokenizer加上Batch=Flase\n",
        "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=False)"
      ],
      "metadata": {
        "id": "X-9Gmr-8cM0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快速tokenizer，一般不指定参数直接下载的都是快速tokenizer\n",
        "# 比较快速/慢速tokenizer和加不加batch对同一文本的编码速度\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 慢速\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True)"
      ],
      "metadata": {
        "id": "b_pn2YYVdK6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 慢速tokenizer加上Batch=True\n",
        "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "5JJM4zcRefBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 慢速tokenizer加上Batch=False\n",
        "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=False)"
      ],
      "metadata": {
        "id": "VAK86BYUeiQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快速tokenizer，一般不指定参数直接下载的都是快速tokenizer\n",
        "# 比较快速/慢速tokenizer和加不加batch对同一文本的编码速度\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"review\"], truncation=True)"
      ],
      "metadata": {
        "id": "8v_Op0oggHa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset"
      ],
      "metadata": {
        "id": "IbdjuzG5j7g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 快速tokenizer加上batch加多进程\n",
        "# 不推荐在快速tokenizer和batch的情况下使用多进程，没必要\n",
        "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True, num_proc=8)"
      ],
      "metadata": {
        "id": "ChJz696hjD3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(drug_dataset))\n",
        "print(drug_dataset)"
      ],
      "metadata": {
        "id": "X3MYeW18jMsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从一个样本中创建几个训练特征\n",
        "def tokenize_and_split(examples):\n",
        "  return tokenizer(\n",
        "      examples[\"review\"],\n",
        "      truncation=True,\n",
        "      max_length=128,\n",
        "      # 返回全部文本块而不是丢弃只保留第一个，按128切分了\n",
        "      return_overflowing_tokens=True\n",
        "  )"
      ],
      "metadata": {
        "id": "gkueDtL0ktzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上面是在一个数据上的测试结果\n",
        "# 一个样本变成了两个特征，一个长度128，一个长度49\n",
        "[len(inp) for inp in result[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "YQ2778u9lILA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "qBp6cFdwkPcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 详细解释一下下面的报错\n",
        "# Dataset是一个包含了很多字典的表，类似\n",
        "# 行0: {review: \"...\", label: 1}\n",
        "# 行1: {review: \"...\", label: 0}\n",
        "# 行2: {review: \"...\", label: 1}\n",
        "# ...\n",
        "# 选择batched=True的时候，map函数看到的就变成\n",
        "# batch = {\n",
        "#   \"review\": [\"r0\", \"r1\", \"r2\", ..., \"r999\"],   # 长度 = N\n",
        "#   \"label\":  [  1,    0,    1,  ...,    0   ]   # 长度 = N\n",
        "# }\n",
        "# 你写的map函数，就是给你一个batch，我需要返回一个dict，map函数会把我返回的dict和原来的dataset拼在一起\n",
        "# 所以加入我返回了一个新列的列表，就是key:value(list)是一一对应的，比如\n",
        "# return {\n",
        "#   \"new_feature\": [\"x0\", \"x1\", \"x2\", ..., \"x999\"]\n",
        "# }\n",
        "# 那么最后得到的Dataset就是正常的，比如\n",
        "# 行0: {review: r0, new_feature: x0}\n",
        "# 行1: {review: r1, new_feature: x1}\n",
        "# ...\n",
        "# 所以现在的问题是，由于前面选择了不丢弃超出长度的token，导致map调用的函数返回的长度和dataset的数据条目不匹配了，也就是\n",
        "# return {\n",
        "#   \"input_ids\": [...长度1463...],\n",
        "#   \"attention_mask\": [...长度1463...]\n",
        "# }\n",
        "# 所以map函数想把我返回的input_ids和attention_mask拼接回去Dataset的时候报错\n",
        "# 明明一批应该只有1000个，对应1000个新属性：新值，为什么现在又1463个"
      ],
      "metadata": {
        "id": "Y3Sc2eworoyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
      ],
      "metadata": {
        "id": "isDCMfNulbCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 问题就是我们想混合两个长度不同的数据集\n",
        "# 那么解决办法就是，要么删除旧数据集的列（其实就是丢弃原来的数据），要么它们和新数据集里面的尺寸相同\n",
        "# 这里实现前者\n",
        "tokenized_dataset = drug_dataset.map(\n",
        "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "j_6YmnW1lRFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理之后的新数据集比原始的要大，因为我们没有丢弃被拆分的token\n",
        "len(tokenized_dataset[\"train\"]),len(drug_dataset[\"train\"])"
      ],
      "metadata": {
        "id": "0eiTqg3PlSGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 也可以选择把旧的列变成和新列保持相同大小\n",
        "# 用overflow_to_sample_mapping，给出新特征索引到它源自的样本索引的映射\n",
        "def tokenize_and_split(examples):\n",
        "    result = tokenizer(\n",
        "        examples[\"review\"],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_overflowing_tokens=True,\n",
        "    )\n",
        "    # 提取新旧索引之间的映射\n",
        "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
        "    for key, values in examples.items():\n",
        "        result[key] = [values[i] for i in sample_map]\n",
        "    return result"
      ],
      "metadata": {
        "id": "i9m8jyvOvzz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
      ],
      "metadata": {
        "id": "tWm__WgGweTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "LSn-znIEwcy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets和DataFrames的相互转换\n",
        "drug_dataset"
      ],
      "metadata": {
        "id": "X4cRrPCiw5Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset.set_format(\"pandas\")"
      ],
      "metadata": {
        "id": "SBoKiOQAK2CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset"
      ],
      "metadata": {
        "id": "ZfRnB2gsK7KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"][:3]"
      ],
      "metadata": {
        "id": "Z9cXb8pJK8NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drug_dataset[\"train\"]依旧是Dataset格式，set_format只改变了__getitem__()方法的返回格式\n",
        "\n",
        "drug_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "OhNCruZrLAA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 需要对整个数据集切片才可以获得pandas.DataFrame格式的数据集\n",
        "train_df = drug_dataset[\"train\"][:]"
      ],
      "metadata": {
        "id": "xgBjzuhpLLRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "hGnX9ZmnLeef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequencies = (\n",
        "    train_df[\"condition\"] # 找到这个表的condition这一列\n",
        "    .value_counts() # 统计这个列里面每个值出现的次数\n",
        "    .to_frame() # 把上面统计的结果转换成表\n",
        "    .reset_index() # 给这个表加上index索引，就是下面的样子\n",
        "    .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"}) # 给表的列改名\n",
        ")\n",
        "frequencies.head()"
      ],
      "metadata": {
        "id": "T9P1MrReLfPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset.from_pandas()创建一个新的Dataset对象\n",
        "from datasets import Dataset\n",
        "\n",
        "freq_dataset = Dataset.from_pandas(frequencies)\n",
        "freq_dataset"
      ],
      "metadata": {
        "id": "cDbLtcwjOcsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset"
      ],
      "metadata": {
        "id": "hMW3DqtyO8dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "suNGuz9IO-3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "QEopfBtVPKsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 试试看！\n",
        "drug_avg = (\n",
        "    train_df\n",
        "    .groupby(\"drugName\")[\"rating\"]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "drug_avg.head()"
      ],
      "metadata": {
        "id": "-1Gj1fayPOL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_avg_dataset = Dataset.from_pandas(drug_avg)\n",
        "drug_avg_dataset"
      ],
      "metadata": {
        "id": "e7sY75sTPpj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 重置成Arrow\n",
        "drug_dataset.reset_format()"
      ],
      "metadata": {
        "id": "9_3ryWucP0WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建验证集合\n",
        "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(\n",
        "    train_size = 0.8,\n",
        "    seed = 42\n",
        ")"
      ],
      "metadata": {
        "id": "aBg-bucRQXOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset_clean"
      ],
      "metadata": {
        "id": "jAxk2cKdSGti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 改名，把test改成validation\n",
        "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")"
      ],
      "metadata": {
        "id": "S5Lw03AzSIHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset_clean"
      ],
      "metadata": {
        "id": "bIvo3O_sSQYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]"
      ],
      "metadata": {
        "id": "08FmBXb_SQ_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drug_dataset_clean"
      ],
      "metadata": {
        "id": "FotsePXtSZoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存数据集，可以保存Arrow，CSV，JSON格式的\n",
        "# Arrow格式\n",
        "drug_dataset_clean.save_to_disk(\"drug_reviews\")"
      ],
      "metadata": {
        "id": "UKtw5OBfSaXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "_LU4UzeFS03g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./drug_reviews/"
      ],
      "metadata": {
        "id": "dLnOfRS_S3Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./drug_reviews/test"
      ],
      "metadata": {
        "id": "JnoG_Wk-S7Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_from_disk()从磁盘读取数据\n",
        "from datasets import load_from_disk\n",
        "\n",
        "drug_dataset_reloaded = load_from_disk(\"drug_reviews\")\n",
        "drug_dataset_reloaded"
      ],
      "metadata": {
        "id": "fObC5CaFS-HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1GRtJ_ccTOBU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}