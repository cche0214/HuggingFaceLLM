{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb6FE7FZ3VGpErV939Cb4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cche0214/HuggingFaceLLM/blob/main/02pipeline%E7%9A%84%E5%86%85%E9%83%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5hX4Orcy4ss"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipeline集成了三个步骤，预处理，模型计算和后处理\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 检查点是针对于架构的权重，这里的架构就是DistilBERT\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "# 这里的意思就是，找到这个检查点下的Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "dZtMZvsc08zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\"\n",
        "]\n",
        "\n",
        "# Tokenizer将原始的文本转换成tokenID（实际应该是原始文本->分词成token->token映射为tokenID\n",
        "# return_tensors参数指定返回的tensor类型，可以是PyTorch、TensorFlow或者纯NumPy\n",
        "# Transformers模型只接受张量输入\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "CRGMhi-s23cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出包含两个键input_ids和attention_mask\n",
        "# input_ids就是每个句子中token的ID\n",
        "# 现在就是有了可以输入模型的数据，所以接下来下载模型\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "lzrg0hoJ3ntR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# 这里并没有加载模型头，也就是不能得到情感分类任务的结果\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "gjZbjdZl4Csx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "\n",
        "# 输入句子，输出称为hidden states隐状态，这个维度很大有三个维度，隐状态也被称为模型头（输入下游任务）\n",
        "# [BatchSize, SequenceLength, Hiddensize]\n",
        "# 分别是一次处理的序列数量，表示序列（句子）的长度和每个模型输入的向量维度\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "bSzXsKor4rHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "I9WBhCVy5te7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 现在要导入一个带有序列分类头的模型，也就是情感分类模型\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# 这里的checkpoint跟单纯导入AutoModle一样，原因是两者的区别只是有没有下游的模型头\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n"
      ],
      "metadata": {
        "id": "t1wEQ5Wz5pER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这里的维度小了很多，因为模型头已经接受了前面的高维向量作为输入，输出包含两个值（每种标签一个的）向量\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "id": "ftbcEikl7sLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "TC-jm85d75tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 预测出来的暂时还是对数几率，是模型最后一层输出的原始的、未标准化的分数\n",
        "# 因此要知道最后的概率，需要经过softmax层，所有Transformers模型的输出都是logits\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "QFmRlYs976kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 经过softmax层之后才得到概率，现在需要查看模型看每个概率对应哪种标签\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "qBqSZ5YU8L8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 现在知道第一个是负面的概率，第二个是积极的概率\n",
        "model.config.id2label"
      ],
      "metadata": {
        "id": "paj7NZlq8tF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试更多的句子，分别比较自己的管道和transformers实现的管道\n",
        "from transformers import pipeline\n",
        "\n",
        "classifierTF = pipeline(\"sentiment-analysis\")\n",
        "classifierTF(\n",
        "    [\n",
        "        \"I absolutely love this project—it exceeded every expectation I had.\",\n",
        "        \"This is the most rewarding experience I’ve had in years.\",\n",
        "        \"I’m extremely disappointed with the final outcome.\",\n",
        "        \"The entire process was frustrating and poorly managed.\",\n",
        "        \"The outcome is good, and it meets most of my expectations.\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OtOYAfnz83VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "tkADkZBNSLpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "    \"I absolutely love this project—it exceeded every expectation I had.\",\n",
        "    \"This is the most rewarding experience I’ve had in years.\",\n",
        "    \"I’m extremely disappointed with the final outcome.\",\n",
        "    \"The entire process was frustrating and poorly managed.\",\n",
        "    \"The outcome is good, and it meets most of my expectations.\"\n",
        "]\n",
        "\n",
        "inputsid = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputsid)"
      ],
      "metadata": {
        "id": "pBg44lE5SmA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "7JGRKxM-S_-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这里前面加上**是Python语法，表示把inputsid这个字典的每个key作为函数的输入\n",
        "# outs = model(input_ids = inputsid[\"inputs_ids\"], attention_mask = inputsid[\"attention_mask\"])\n",
        "outs = model(**inputsid)\n",
        "\n",
        "# 模型最后一层输出，没有经过SoftMax层\n",
        "print(outs)"
      ],
      "metadata": {
        "id": "Xeh3H9G9TImL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outs.logits, dim=-1)\n",
        "print(predictions)\n",
        "# 输出和直接用pipeline是一样的"
      ],
      "metadata": {
        "id": "UBeXOlGuUCmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "y7J4oKH7UV98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "\n",
        "# 使用默认配置创建模型会使用随机值对其进行初始化\n",
        "# 模型可以运行得到结果，但是输出会胡言乱语\n",
        "model = BertModel(config)"
      ],
      "metadata": {
        "id": "3M2VDwfIUZD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config中包含许多用于构建模型的属性，是写死的，还没有训练\n",
        "# 还没有训练的意思是，它的权重还是随机的，config展示的只是它结构的一些属性\n",
        "# hidden_states隐状态向量的大小，num_hidden_layers定义了Transformers模型的参数\n",
        "print(config)"
      ],
      "metadata": {
        "id": "_0ONSc3lXm29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 未训练的模型输出是胡言乱语，不过这个是基础模型我不知道怎么确认\n",
        "outputs = model(**inputsid)\n",
        "print(outputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "t8ds5apXXya7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载已经训练过的模型，参考上一节的方法\n",
        "from transformers import BertModel\n",
        "\n",
        "# 这里和AutoModel的区别就是，你已经指定了Bert模型的架构，所以你的checkpoint必须是Bert模型的\n",
        "# 所以这里的逻辑就是找到Bert模型的bert-base-cased检查点的模型\n",
        "# 这个模型就是预训练后的模型了，可以执行任务，也可以在新任务上微调\n",
        "# 因此从单个Model变成AutoModel，checkpoint就不受限制了，Transformers库会自动识别\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "gXhSdrHfZiJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "rZsSzrj7bSmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls directory_on_my_computer\n",
        "# config.json表示模型的结构和元数据\n",
        "# model.safetensors表示模型的所有参数"
      ],
      "metadata": {
        "id": "TeVpeYlycE1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从自己的电脑下载模型\n",
        "modelload = BertModel.from_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "0NZZ2p4AcIDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "XA8FVfJJcp8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这里就是指定BERT模型的分词器\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "UCeQY5jlc4a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 同样的，你可以直接使用AutoTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "cMPwp3gOpGd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformers network is simple\")\n",
        "# 这个输出和情感分类的那个模型输出好像不同，多了一个token_type_ids"
      ],
      "metadata": {
        "id": "zqpQPXWTpRnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "1Ecu3OzYpf4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformers network is simple\")\n",
        "# 情感分类的输出"
      ],
      "metadata": {
        "id": "hMy-yJvzp0pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存tokenizer\n",
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "MSMlJ7vZp2v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 详细展示Tokenizer内部的处理过程:原始文本->分词（不同模型的分词法不同）->映射词汇ID（词汇表)->转换成模型可以接受的张量\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 实例化一个BERT模型的分词器，你正常使用直接用就行，下面展示详细内部步骤\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformers network is simple, I like tokenization\"\n",
        "print(sequence)\n",
        "\n",
        "# 第一步分词，也就是把文本变成Token\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "print(tokens) # 这里也可以看出来,BERT模型是子词分词\n",
        "\n",
        "# 第二步token查表变成inputIDs\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "# 第三步还要给ids进行处理，加上模型需要的特殊字符\n",
        "final_inputs = tokenizer.prepare_for_model(ids)\n",
        "print(final_inputs)\n",
        "\n",
        "# 解码过程decode\n",
        "# 不仅将索引转回tokens，还会将相同单词的tokens组合在一起生成可读的句子\n",
        "decoded_string = tokenizer.decode(ids)\n",
        "print(decoded_string)\n",
        "\n",
        "\n",
        "# 经过这三步生成的就跟直接调用tokenizer一样了\n",
        "# ！！注意这句话有问题，经过上面那三步，没有把它转换成tensor的形式，所以不能喂给模型！\n",
        "inputsid = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputsid)"
      ],
      "metadata": {
        "id": "KFOtt5-CqK32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 试试看！\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "inputs = [\n",
        "    \"I’ve been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\"\n",
        "]\n",
        "print(inputs)\n",
        "\n",
        "tokens = tokenizer.tokenize(inputs)\n",
        "print(tokens)\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "yIGIDDw_sARc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型需要一批输入\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "# 转换出来的向量是一维的\n",
        "input_ids = torch.tensor(ids)\n",
        "print(input_ids)\n",
        "\n",
        "# 我们直接用tokenizer看看，到底喂给模型的是什么\n",
        "tokenized_inputs = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# 看到了吗，他是两维的\n",
        "print(tokenized_inputs[\"input_ids\"])\n",
        "\n",
        "final_inputs = torch.tensor([ids])\n",
        "print(final_inputs)"
      ],
      "metadata": {
        "id": "-l7TwS6qtkIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 会报错，只输入了一个句子但是模型需要一个句子列表\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "# 增加一个维度，数据就输入成功了，复习一下怎么把他转变回来吧！\n",
        "out = model(final_inputs)\n",
        "print(out)\n",
        "\n",
        "predictions = torch.nn.functional.softmax(out.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "0hpUao2lz-Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 前几天学习第二章的处理多个序列昏头了，重新来一次\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "\n",
        "prepared_input = tokenizer.prepare_for_model(ids)\n",
        "\n",
        "print(input_ids)\n",
        "print(prepared_input) # 注意和之间转换差tensor\n",
        "print(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\"))"
      ],
      "metadata": {
        "id": "qJ9JoeZQ0OO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])\n",
        "# 和上面比多了一个维度"
      ],
      "metadata": {
        "id": "QX32W1lmHELb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "# 手动的话如果没有prepare_for_model，你会发现他也没有特殊token\n",
        "out = model(input_ids)\n",
        "print(\"out:\", out[\"logits\"])"
      ],
      "metadata": {
        "id": "qsHPLWEMHwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果你只有一句话，你可以构建只有一个句子的batch\n",
        "# 试试看！\n",
        "batched_ids = [ids, ids]\n",
        "input_ids = torch.tensor(batched_ids)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "out = model(input_ids)\n",
        "print(\"out:\", out[\"logits\"])"
      ],
      "metadata": {
        "id": "0tviHN_LIX2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 张量要是矩形，所以\n",
        "# batched_ids = [\n",
        "#     [200, 200, 200],\n",
        "#     [200, 200]\n",
        "# ]\n",
        "# 不能转换成张量 -> 填充输入Padding\n",
        "\n",
        "padding_id = 100\n",
        "# padding的作用是在值较少的句子中添加一个名为padding_id的特殊单词确保所有句子长度相同\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id]\n",
        "]\n"
      ],
      "metadata": {
        "id": "FCKber7xI5LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.pad_token_id找到分词器的填充token的ID\n",
        "print(tokenizer.pad_token_id) # 输出为0"
      ],
      "metadata": {
        "id": "rHSjpyq7Jmv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 比较单个处理和批处理，并使用填充的tokenID\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id]\n",
        "]\n",
        "\n",
        "# 第一个结果并不是前两个结果的简单拼起来，为什么？\n",
        "# 注意力层会考虑每个token的上下文信息\n",
        "# 因此通过pad填充之后，填充的这个值也被注意力层纳入考虑范围，尽管它没有什么实际含义\n",
        "# 需要通过注意力掩码attention_mask层来让注意力层忽略这些填充的token\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "# 这里就是没有加上注意力掩码层的结果，所有token都被考虑了，所以输出不同\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ],
      "metadata": {
        "id": "7yXi8jsyJ2Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id]\n",
        "]\n",
        "\n",
        "# 与Input_IDs张量形状完全一样的张量，用0和1填充，1表示应该关注，0表示忽略\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0]\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask = torch.tensor(attention_mask))\n",
        "print(outputs)\n",
        "# 现在的值就和上面两个句子单独考虑的一样了，说明忽略了填充的token\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "yPUISDAwK4zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 试试看！\n",
        "sequence_1 = \"I’ve been waiting for a HuggingFace course my whole life.\"\n",
        "sequence_2 = \"I hate this so much!\"\n",
        "\n",
        "tokens_1 = tokenizer.tokenize(sequence_1)\n",
        "tokens_2 = tokenizer.tokenize(sequence_2)\n",
        "\n",
        "input_ids1 = tokenizer.convert_tokens_to_ids(tokens_1)\n",
        "input_ids2 = tokenizer.convert_tokens_to_ids(tokens_2)\n",
        "\n",
        "input_ids22 = input_ids2 + [tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id, tokenizer.pad_token_id, ]\n",
        "\n",
        "batched_ids = [\n",
        "    input_ids1,\n",
        "    input_ids22\n",
        "]\n",
        "\n",
        "print(batched_ids)\n",
        "\n",
        "input_ids = torch.tensor(batched_ids)\n",
        "\n",
        "print(input_ids)\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "]\n",
        "\n",
        "print(torch.tensor(attention_mask))\n",
        "\n",
        "outputs = model(input_ids, attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)\n"
      ],
      "metadata": {
        "id": "LB8IVWtpMDR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_1 = tokenizer(sequence_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(output_1)\n",
        "answer = model(**output_1)\n",
        "print(answer) # 这里不一样是因为我没有加上特殊token"
      ],
      "metadata": {
        "id": "--u9OZJXM2j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 综合应用\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ],
      "metadata": {
        "id": "_FbAoFEnQS95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "ju-Rd8l_vRwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"So have I!\"\n",
        "]\n",
        "\n",
        "model_inputs = tokenizer(sequences)"
      ],
      "metadata": {
        "id": "DBRV7ZbrvZe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "BD55I6nxveNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "quEH5dqTvgYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "3Jonbt_EvxSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "GpF8LFzNv9QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "eWZh0T7wwIUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequences, truncation=True, max_length=8)\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "cWQHvSd4wS5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequence, padding=True, return_tensors=\"pt\")\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "83a33TcLwbnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# 和直接用不同，直接调用Tokenizer会帮你加上开头结尾的特殊字符\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "b0a_6w13wqaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "eVUVikXhwyrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "out = model(**model_inputs)\n",
        "print(out)\n",
        "print(out.logits)\n",
        "\n",
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(out.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "Kinx5KDBxMb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classificater = pipeline(\"sentiment-analysis\")\n",
        "out = classificater(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"So have I!\"\n",
        "    ]\n",
        ")\n",
        "print(out)"
      ],
      "metadata": {
        "id": "r-gkclUEx73q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vIYzUB6yqze"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}